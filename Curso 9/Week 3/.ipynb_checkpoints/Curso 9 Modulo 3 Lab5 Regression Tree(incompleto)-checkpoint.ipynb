{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3207da-8d64-4590-b51f-dee8ce4ecba9",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a0d89c-1c35-4207-9d19-01a89f3858a3",
   "metadata": {},
   "source": [
    "# **Taxi Tip Prediction using Scikit-Learn and Snap ML**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7042fb2f-8729-4e2b-ba4e-c0661a0b0287",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9693efe7-6155-45aa-88bf-24324ad6c2e6",
   "metadata": {},
   "source": [
    "In this exercise session you will consolidate your machine learning (ML) modeling skills by using a popular regression model: Decision Tree. You will use a real dataset to train such a model. The dataset includes information about taxi tip and was collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP). You will use the trained model to predict the amount of tip paid. \n",
    "\n",
    "In the current exercise session, you will practice not only the Scikit-Learn Python interface, but also the Python API offered by the Snap Machine Learning (Snap ML) library. Snap ML is a high-performance IBM library for ML modeling. It provides highly-efficient CPU/GPU implementations of linear models and tree-based models. Snap ML not only accelerates ML algorithms through system awareness, but it also offers novel ML algorithms with best-in-class accuracy. For more information, please visit [snapml](https://ibm.biz/BdPfxy) information page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca9d2b-97fe-44b7-a997-3731a8359775",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb28cf-ccdd-458b-b74c-589053005000",
   "metadata": {},
   "source": [
    "After completing this lab you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a4906d-4165-45e9-93a9-a7b04b6cc3c7",
   "metadata": {},
   "source": [
    "* Perform basic data preprocessing using Scikit-Learn\n",
    "* Model a regression task using the Scikit-Learn and Snap ML Python APIs\n",
    "* Train a Decision Tree Regressor model using Scikit-Learn and Snap ML\n",
    "* Run inference and assess the quality of the trained models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d86e14-9a38-489c-b9e8-b32c9b43919d",
   "metadata": {},
   "source": [
    "## Table of Contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d873f-e12f-421e-953c-fddc7326d177",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 10px\">\n",
    "    <ol>\n",
    "        <li><a href=\"#introduction\">Introduction</a></li>\n",
    "        <li><a href=\"#import_libraries\">Import Libraries</a></li>\n",
    "        <li><a href=\"#dataset_analysis\">Dataset Analysis</a></li>\n",
    "        <li><a href=\"#dataset_preprocessing\">Dataset Preprocessing</a></li>\n",
    "        <li><a href=\"#dataset_split\">Dataset Train/Test Split</a></li>\n",
    "        <li><a href=\"#dt_sklearn\">Build a Decision Tree Regressor model with Scikit-Learn</a></li>\n",
    "        <li><a href=\"#dt_snap\">Build a Decision Tree Regressor model with Snap ML</a></li>\n",
    "        <li><a href=\"#dt_sklearn_snap\">Evaluate the Scikit-Learn and Snap ML Decision Tree Regressors</a></li>\n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44eccbd-a975-4eb8-a026-559f40673b4a",
   "metadata": {},
   "source": [
    "<div id=\"Introduction\">\n",
    "    <h2>Introduction</h2>\n",
    "    <br>The dataset used in this exercise session is publicly available here: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page (all rights reserved by Taxi & Limousine Commission(TLC), City of New York). The TLC Yellow Taxi Trip Records of June, 2019 are used in this notebook. The prediction of the tip amount can be modeled as a regression problem. To train the model you can use part of the input dataset and the remaining data can be used to assess the quality of the trained model. First, let's download the dataset.\n",
    "    <br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16234833-d58c-4275-bf38-3c63c804efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download June 2020 TLC Yellow Taxi Trip records\n",
    "# !wget -nc https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-06.csv\n",
    "path = 'cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/yellow_tripdata_2019-06.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf199bb-90e3-4226-bca8-65f02dff7c94",
   "metadata": {},
   "source": [
    "__Did you know?__ When it comes to Machine Learning, you will most likely be working with large datasets. As a business, where can you host your data? IBM is offering a unique opportunity for businesses, with 10 Tb of IBM Cloud Object Storage: [Sign up now for free](https://ibm.biz/BdPfxf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e720ac9-c78f-4934-b9ee-39499ac8b45e",
   "metadata": {},
   "source": [
    "<div id=\"import_libraries\">\n",
    "    <h2>Import Libraries</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "519b5e64-e01a-4452-8364-f2399f89dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snap ML is available on PyPI. To install it simply run the pip command below.\n",
    "# !pip install snapml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4a40d7-89b7-4d4e-b558-49f3bba92e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we need to use in this lab\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize, StandardScaler, MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import warnings\n",
    "import gc, sys\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4058009-4b39-48ad-8bc8-c39f16e0f57f",
   "metadata": {},
   "source": [
    "<div id=\"dataset_analysis\">\n",
    "    <h2>Dataset Analysis</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7a3459-c5e8-4aa7-9957-277f5b9fde7b",
   "metadata": {},
   "source": [
    "In this section you will read the dataset in a Pandas dataframe and visualize its content. You will also look at some data statistics.\n",
    "\n",
    "Note: A Pandas dataframe is a two-dimensional, size-mutable, potentially heterogeneous tabular data structure. For more information: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a305f6b-c5eb-4fee-a8ed-1cc516f6ef2e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/yellow_tripdata_2019-06.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# read the input data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m raw_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(raw_data)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m observations in the dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(raw_data\u001b[38;5;241m.\u001b[39mcolumns)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m variables in the dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/yellow_tripdata_2019-06.csv'"
     ]
    }
   ],
   "source": [
    "# read the input data\n",
    "raw_data = pd.read_csv(path)\n",
    "print(\"There are \" + str(len(raw_data)) + \" observations in the dataset.\")\n",
    "print(\"There are \" + str(len(raw_data.columns)) + \" variables in the dataset.\")\n",
    "\n",
    "# display first rows in the dataset\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f0c52-4dac-4c28-8035-605d904404bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reducing the data size to 100000 records\n",
    "raw_data=raw_data.head(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d3f8fa-5db1-45f6-a4ea-95560e151717",
   "metadata": {},
   "source": [
    "Each row in the dataset represents a taxi trip. As shown above, each row has 18 variables. One variable is called tip_amount and represents the target variable. Your objective will be to train a model that uses the other variables to predict the value of the tip_amount variable. Let's first clean the dataset and retrieve basic statistics about the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d033bb-7743-4b12-af09-3ad3182958f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some trips report 0 tip. it is assumed that these tips were paid in cash.\n",
    "# for this study we drop all these rows\n",
    "raw_data = raw_data[raw_data['tip_amount'] > 0]\n",
    "\n",
    "# we also remove some outliers, namely those where the tip was larger than the fare cost\n",
    "raw_data = raw_data[(raw_data['tip_amount'] <= raw_data['fare_amount'])]\n",
    "\n",
    "# we remove trips with very large fare cost\n",
    "raw_data = raw_data[((raw_data['fare_amount'] >=2) & (raw_data['fare_amount'] < 200))]\n",
    "\n",
    "# we drop variables that include the target variable in it, namely the total_amount\n",
    "clean_data = raw_data.drop(['total_amount'], axis=1)\n",
    "\n",
    "# release memory occupied by raw_data as we do not need it anymore\n",
    "# we are dealing with a large dataset, thus we need to make sure we do not run out of memory\n",
    "del raw_data\n",
    "gc.collect()\n",
    "\n",
    "# print the number of trips left in the dataset\n",
    "print(\"There are \" + str(len(clean_data)) + \" observations in the dataset.\")\n",
    "print(\"There are \" + str(len(clean_data.columns)) + \" variables in the dataset.\")\n",
    "\n",
    "plt.hist(clean_data.tip_amount.values, 16, histtype='bar', facecolor='g')\n",
    "plt.show()\n",
    "\n",
    "print(\"Minimum amount value is \", np.min(clean_data.tip_amount.values))\n",
    "print(\"Maximum amount value is \", np.max(clean_data.tip_amount.values))\n",
    "print(\"90% of the trips have a tip amount less or equal than \", np.percentile(clean_data.tip_amount.values, 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f9645-7caf-473f-9f6d-214be552ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first rows in the dataset\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1700fbc7-d701-4094-aad8-3acc66a6bf2a",
   "metadata": {},
   "source": [
    "By looking at the dataset in more detail, we see that it contains information such as pick-up and drop-off dates/times, pick-up and drop-off locations, payment types, driver-reported passenger counts etc. Before actually training a ML model, we will need to preprocess the data. We need to transform the data in a format that will be correctly handled by the models. For instance, we need to encode the categorical features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21b6b18-c18c-4a6f-a2a1-f53d604831f3",
   "metadata": {},
   "source": [
    "<div id=\"dataset_preprocessing\">\n",
    "    <h2>Dataset Preprocessing</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ff75d-f816-4dc2-aafc-0920ad063357",
   "metadata": {},
   "source": [
    "In this subsection you will prepare the data for training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1066f-682c-4b64-85fd-255686044977",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['tpep_dropoff_datetime'] = pd.to_datetime(clean_data['tpep_dropoff_datetime'])\n",
    "clean_data['tpep_pickup_datetime'] = pd.to_datetime(clean_data['tpep_pickup_datetime'])\n",
    "\n",
    "# extract pickup and dropoff hour\n",
    "clean_data['pickup_hour'] = clean_data['tpep_pickup_datetime'].dt.hour\n",
    "clean_data['dropoff_hour'] = clean_data['tpep_dropoff_datetime'].dt.hour\n",
    "\n",
    "# extract pickup and dropoff day of week\n",
    "clean_data['pickup_day'] = clean_data['tpep_pickup_datetime'].dt.weekday\n",
    "clean_data['dropoff_day'] = clean_data['tpep_dropoff_datetime'].dt.weekday\n",
    "\n",
    "# compute trip time in minutes\n",
    "clean_data['trip_time'] = (clean_data['tpep_dropoff_datetime'] - clean_data['tpep_pickup_datetime']).astype('timedelta64[m]')\n",
    "\n",
    "# ideally use the full dataset for this exercise\n",
    "# however if you run into out of memory issues due to the data size, reduce it\n",
    "# for instance, in this example we use only the first 1M samples\n",
    "first_n_rows = 1000000\n",
    "clean_data = clean_data.head(first_n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac543e6-29b7-468f-a38e-dde4ac770faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the pickup and dropoff datetimes\n",
    "clean_data = clean_data.drop(['tpep_pickup_datetime', 'tpep_dropoff_datetime'], axis=1)\n",
    "\n",
    "# some features are categorical, we need to encode them\n",
    "# to encode them we use one-hot encoding from the Pandas package\n",
    "get_dummy_col = [\"VendorID\",\"RatecodeID\",\"store_and_fwd_flag\",\"PULocationID\", \"DOLocationID\",\"payment_type\", \"pickup_hour\", \"dropoff_hour\", \"pickup_day\", \"dropoff_day\"]\n",
    "proc_data = pd.get_dummies(clean_data, columns = get_dummy_col)\n",
    "\n",
    "# release memory occupied by clean_data as we do not need it anymore\n",
    "# we are dealing with a large dataset, thus we need to make sure we do not run out of memory\n",
    "del clean_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c758184-78d1-4231-9e41-ffaa1cded372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the labels from the dataframe\n",
    "y = proc_data[['tip_amount']].values.astype('float32')\n",
    "\n",
    "# drop the target variable from the feature matrix\n",
    "proc_data = proc_data.drop(['tip_amount'], axis=1)\n",
    "\n",
    "# get the feature matrix used for training\n",
    "X = proc_data.values\n",
    "\n",
    "# normalize the feature matrix\n",
    "X = normalize(X, axis=1, norm='l1', copy=False)\n",
    "\n",
    "# print the shape of the features matrix and the labels vector\n",
    "print('X.shape=', X.shape, 'y.shape=', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7b00b7-2322-4f47-bc44-d9760157f5d5",
   "metadata": {},
   "source": [
    "<div id=\"dataset_split\">\n",
    "    <h2>Dataset Train/Test Split</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aacb94e-09a5-4336-bddf-463215a5e1a7",
   "metadata": {},
   "source": [
    "Now that the dataset is ready for building the classification models, you need to first divide the pre-processed dataset into a subset to be used for training the model (the train set) and a subset to be used for evaluating the quality of the model (the test set).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b908278-90ce-49ca-b11e-e07b135dd584",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print('X_train.shape=', X_train.shape, 'Y_train.shape=', y_train.shape)\n",
    "print('X_test.shape=', X_test.shape, 'Y_test.shape=', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba691fd3-6f65-46de-9872-4d397be229b4",
   "metadata": {},
   "source": [
    "<div id=\"dt_sklearn\">\n",
    "    <h2>Build a Decision Tree Regressor model with Scikit-Learn</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bc8533-6065-4bf6-8b41-e3c004729b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Decision Tree Regression Model from scikit-learn\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# for reproducible output across multiple function calls, set random_state to a given integer value\n",
    "sklearn_dt = DecisionTreeRegressor(max_depth=8, random_state=35)\n",
    "\n",
    "# train a Decision Tree Regressor using scikit-learn\n",
    "t0 = time.time()\n",
    "sklearn_dt.fit(X_train, y_train)\n",
    "sklearn_time = time.time()-t0\n",
    "print(\"[Scikit-Learn] Training time (s):  {0:.5f}\".format(sklearn_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e198b-7856-46a7-b0c8-74a391c1f730",
   "metadata": {},
   "source": [
    "<div id=\"dt_snapml\">\n",
    "    <h2>Build a Decision Tree Regressor model with Snap ML</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04c0b8-f11f-48b6-b708-d341da044552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Decision Tree Regressor Model from Snap ML\n",
    "from snapml import DecisionTreeRegressor\n",
    "\n",
    "# in contrast to sklearn's Decision Tree, Snap ML offers multi-threaded CPU/GPU training \n",
    "# to use the GPU, one needs to set the use_gpu parameter to True\n",
    "# snapml_dt = DecisionTreeRegressor(max_depth=4, random_state=45, use_gpu=True)\n",
    "\n",
    "# to set the number of CPU threads used at training time, one needs to set the n_jobs parameter\n",
    "# for reproducible output across multiple function calls, set random_state to a given integer value\n",
    "snapml_dt = DecisionTreeRegressor(max_depth=8, random_state=45, n_jobs=4)\n",
    "\n",
    "# train a Decision Tree Regressor model using Snap ML\n",
    "t0 = time.time()\n",
    "snapml_dt.fit(X_train, y_train)\n",
    "snapml_time = time.time()-t0\n",
    "print(\"[Snap ML] Training time (s):  {0:.5f}\".format(snapml_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb06001-5cd0-4912-b6c2-06ad5b021c3b",
   "metadata": {},
   "source": [
    "<div id=\"dt_sklearn_snapml\">\n",
    "    <h2>Evaluate the Scikit-Learn and Snap ML Decision Tree Regressor Models</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c0a87-3548-4521-8795-53ba333602e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snap ML vs Scikit-Learn training speedup\n",
    "training_speedup = sklearn_time/snapml_time\n",
    "print('[Decision Tree Regressor] Snap ML vs. Scikit-Learn speedup : {0:.2f}x '.format(training_speedup))\n",
    "\n",
    "# run inference using the sklearn model\n",
    "sklearn_pred = sklearn_dt.predict(X_test)\n",
    "\n",
    "# evaluate mean squared error on the test dataset\n",
    "sklearn_mse = mean_squared_error(y_test, sklearn_pred)\n",
    "print('[Scikit-Learn] MSE score : {0:.3f}'.format(sklearn_mse))\n",
    "\n",
    "# run inference using the Snap ML model\n",
    "snapml_pred = snapml_dt.predict(X_test)\n",
    "\n",
    "# evaluate mean squared error on the test dataset\n",
    "snapml_mse = mean_squared_error(y_test, snapml_pred)\n",
    "print('[Snap ML] MSE score : {0:.3f}'.format(snapml_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba13c08-9c01-4130-a49e-29df844945d3",
   "metadata": {},
   "source": [
    "As shown above both decision tree models provide the same score on the test dataset. However Snap ML runs the training routine faster than Scikit-Learn. This is one of the advantages of using Snap ML: acceleration of training of classical machine learning models, such as linear and tree-based models. For more Snap ML examples, please visit [snapml-examples](https://ibm.biz/BdPfxP). Moreover, as shown above, not only is Snap ML seemlessly accelerating scikit-learn applications, but the library's Python API is also compatible with scikit-learn metrics and data preprocessors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d1f91d-9bd5-44c7-a35e-2490116c06da",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a393c0-635b-4c67-bc92-3ef19bec509e",
   "metadata": {},
   "source": [
    "Andreea Anghel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c6fea-9776-43dc-ade8-5f6ce7480f6c",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a7e364-8ed8-4cf0-bd9b-d65cbe1180a6",
   "metadata": {},
   "source": [
    "Sangeeth Keeriyadath \n",
    "\n",
    "Joseph Santarcangelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f6d8b4-51d9-4ec6-9047-62269377772a",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f373b7e-7db2-4eb0-aa8a-992d376f2556",
   "metadata": {},
   "source": [
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2021-08-31  | 0.1  | AAN  |  Created Lab Content |\n",
    "| 2023-01-24  | 0.2  | Anita Verma  |  Reduced data size|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf2dfa5-e74d-441a-b816-7b2d233b940e",
   "metadata": {},
   "source": [
    " Copyright &copy; 2021 IBM Corporation. This notebook and its source code are released under the terms of the [MIT License](https://cognitiveclass.ai/mit-license/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
